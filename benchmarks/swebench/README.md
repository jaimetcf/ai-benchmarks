# SWEBench Evaluation Examples

This directory contains examples demonstrating how to evaluate code patches on SWEBench tasks using Modal for cloud execution.

## Files Overview

- `swebench.py` - Main SWEBench benchmark implementation with Modal support
- `run_swebench.py` - Comprehensive example with detailed output and error handling
- `simple_example.py` - Minimal example showing core evaluation steps

## Prerequisites

### 1. Install Dependencies

```bash
uv add datasets docker modal
```

### 2. Hugging Face Authentication

The SWEBench dataset requires authentication:

```bash
# Install Hugging Face CLI if not already installed
uv add huggingface_hub

# Login to access the dataset
huggingface-cli login
```

### 3. Modal Setup

For cloud evaluation with Modal:

```bash
# Install Modal
uv add modal

# Set up your Modal token (Settings>API Tokens)
modal token set --token-id <your-token-id> --token-secret <your-token-secret>
```

You can get your Modal tokens from [modal.com](https://modal.com) after creating an account.

### 4. Docker Setup (Optional)

For local evaluation (alternative to Modal):

- Install Docker Desktop
- Ensure Docker daemon is running

## Usage Examples

### Simple Example

The simplest way to evaluate a patch:

```python
from benchmarks.swebench.swebench import SWEBenchVerified

# Initialize benchmark
benchmark = SWEBenchVerified()

# Get a task
tasks = benchmark.get_tasks()
task = tasks[0]

# Your patch code (normally generated by your model)
patch = """--- a/src/file.py
+++ b/src/file.py
@@ -10,7 +10,7 @@ def function():
     if condition:
-        return old_value
+        return new_value
"""

# Evaluate with Modal
result = benchmark.evaluate(code=patch, task=task, use_modal=True)

# Check if successful
if not result.execution.error:
    print("✅ Evaluation completed successfully")
else:
    print(f"❌ Error: {result.execution.error}")
```

### Running the Examples

#### Simple Example
```bash
python -m benchmarks.swebench.simple_example
```

## Understanding SWEBench Tasks

Each SWEBench task contains:

- `instance_id`: Unique identifier (e.g., "django__django-12345")
- `repo`: Repository name (e.g., "django/django")
- `problem_statement`: Description of the issue to fix
- `base_commit`: Git commit hash before the fix
- `patch`: The gold standard solution patch
- `test_patch`: Tests that validate the fix
- `FAIL_TO_PASS`: Tests that should pass after applying your patch
- `PASS_TO_PASS`: Tests that should continue passing

## Patch Format

Patches should be in unified diff format:

```diff
--- a/path/to/file.py
+++ b/path/to/file.py
@@ -line_start,line_count +line_start,line_count @@ context
 unchanged line
-removed line
+added line
 unchanged line
```

## Evaluation Process

When you call `benchmark.evaluate()`, the system:

1. **Sets up environment**: Creates a clean repository environment
2. **Applies patch**: Attempts to apply your patch to the codebase
3. **Runs tests**: Executes the test suite to check if issues are resolved
4. **Reports results**: Returns detailed execution information

## Result Interpretation

The `TaskResult` object contains:

- `task_id`: The evaluated task identifier
- `execution.execution_time`: Time taken for evaluation
- `execution.error`: Any errors that occurred
- `execution.execution_trace`: JSON with detailed results:
  - `patch_applied`: Whether patch was successfully applied
  - `resolved`: Whether the task was solved (tests pass)
  - `tests_status`: Detailed test results

## Modal vs Docker Evaluation

### Modal (Recommended)
- ✅ Cloud-based, no local resource usage
- ✅ Parallel execution capability
- ✅ Consistent environment
- ✅ Better for CI/CD pipelines
- ❌ Requires Modal account and credits

### Docker (Local)
- ✅ No external dependencies
- ✅ Free to use
- ✅ Full control over environment
- ❌ Uses local computational resources
- ❌ Requires Docker setup

## Common Issues and Solutions

### 1. "Import could not be resolved" errors
These are expected linter warnings due to the modular structure. The code will work correctly when the full package is installed.

### 2. Modal authentication errors
```bash
modal token set --token-id <id> --token-secret <secret>
```

### 3. Hugging Face dataset access errors
```bash
huggingface-cli login
```

### 4. Docker permission errors (Linux)
```bash
sudo usermod -aG docker $USER
# Then logout and login again
```

## Example Output

When running the evaluation, you'll see output like:

```
Evaluating task: django__django-12345
Repository: django/django
Starting Modal evaluation...

Results for django__django-12345:
Execution time: 45.2s
✅ Evaluation completed
Patch applied: ✅
Task resolved: ❌
```

## Performance Notes

- Modal evaluation typically takes 30-120 seconds per task
- Local Docker evaluation may be slower depending on your machine
- The first run may take longer due to environment setup
- Large repositories require more time for setup and testing

## Contributing

When adding new examples or modifications:

1. Follow the existing code structure
2. Add appropriate error handling
3. Update this README with new features
4. Test with both Modal and Docker evaluation methods

## Support

For issues with:
- SWEBench dataset: Check [princeton-nlp/SWE-bench](https://github.com/princeton-nlp/SWE-bench)
- Modal platform: Visit [modal.com/docs](https://modal.com/docs)
- Docker setup: Check [docker.com/docs](https://docs.docker.com/) 